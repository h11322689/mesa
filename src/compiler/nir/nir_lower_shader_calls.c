   /*
   * Copyright Â© 2020 Intel Corporation
   *
   * Permission is hereby granted, free of charge, to any person obtaining a
   * copy of this software and associated documentation files (the "Software"),
   * to deal in the Software without restriction, including without limitation
   * the rights to use, copy, modify, merge, publish, distribute, sublicense,
   * and/or sell copies of the Software, and to permit persons to whom the
   * Software is furnished to do so, subject to the following conditions:
   *
   * The above copyright notice and this permission notice (including the next
   * paragraph) shall be included in all copies or substantial portions of the
   * Software.
   *
   * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
   * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
   * IN THE SOFTWARE.
   */
      #include "util/u_dynarray.h"
   #include "util/u_math.h"
   #include "nir.h"
   #include "nir_builder.h"
   #include "nir_phi_builder.h"
      static bool
   move_system_values_to_top(nir_shader *shader)
   {
               bool progress = false;
   nir_foreach_block(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     /* These intrinsics not only can't be re-materialized but aren't
   * preserved when moving to the continuation shader.  We have to move
   * them to the top to ensure they get spilled as needed.
   */
   nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
   switch (intrin->intrinsic) {
   case nir_intrinsic_load_shader_record_ptr:
   case nir_intrinsic_load_btd_local_arg_addr_intel:
      nir_instr_remove(instr);
   nir_instr_insert(nir_before_impl(impl), instr);
               default:
                        if (progress) {
      nir_metadata_preserve(impl, nir_metadata_block_index |
      } else {
                     }
      static bool
   instr_is_shader_call(nir_instr *instr)
   {
      if (instr->type != nir_instr_type_intrinsic)
            nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
   return intrin->intrinsic == nir_intrinsic_trace_ray ||
            }
      /* Previously named bitset, it had to be renamed as FreeBSD defines a struct
   * named bitset in sys/_bitset.h required by pthread_np.h which is included
   * from src/util/u_thread.h that is indirectly included by this file.
   */
   struct sized_bitset {
      BITSET_WORD *set;
      };
      static struct sized_bitset
   bitset_create(void *mem_ctx, unsigned size)
   {
      return (struct sized_bitset){
      .set = rzalloc_array(mem_ctx, BITSET_WORD, BITSET_WORDS(size)),
         }
      static bool
   src_is_in_bitset(nir_src *src, void *_set)
   {
               /* Any SSA values which were added after we generated liveness information
   * are things generated by this pass and, while most of it is arithmetic
   * which we could re-materialize, we don't need to because it's only used
   * for a single load/store and so shouldn't cross any shader calls.
   */
   if (src->ssa->index >= set->size)
               }
      static void
   add_ssa_def_to_bitset(nir_def *def, struct sized_bitset *set)
   {
      if (def->index >= set->size)
               }
      static bool
   can_remat_instr(nir_instr *instr, struct sized_bitset *remat)
   {
      /* Set of all values which are trivially re-materializable and we shouldn't
   * ever spill them.  This includes:
   *
   *   - Undef values
   *   - Constants
   *   - Uniforms (UBO or push constant)
   *   - ALU combinations of any of the above
   *   - Derefs which are either complete or casts of any of the above
   *
   * Because this pass rewrites things in-order and phis are always turned
   * into register writes, we can use "is it SSA?" to answer the question
   * "can my source be re-materialized?". Register writes happen via
   * non-rematerializable intrinsics.
   */
   switch (instr->type) {
   case nir_instr_type_alu:
            case nir_instr_type_deref:
            case nir_instr_type_intrinsic: {
      nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
   switch (intrin->intrinsic) {
   case nir_intrinsic_load_uniform:
   case nir_intrinsic_load_ubo:
   case nir_intrinsic_vulkan_resource_index:
   case nir_intrinsic_vulkan_resource_reindex:
   case nir_intrinsic_load_vulkan_descriptor:
   case nir_intrinsic_load_push_constant:
   case nir_intrinsic_load_global_constant:
   case nir_intrinsic_load_global_const_block_intel:
   case nir_intrinsic_load_desc_set_address_intel:
      /* These intrinsics don't need to be spilled as long as they don't
   * depend on any spilled values.
               case nir_intrinsic_load_scratch_base_ptr:
   case nir_intrinsic_load_ray_launch_id:
   case nir_intrinsic_load_topology_id_intel:
   case nir_intrinsic_load_btd_global_arg_addr_intel:
   case nir_intrinsic_load_btd_resume_sbt_addr_intel:
   case nir_intrinsic_load_ray_base_mem_addr_intel:
   case nir_intrinsic_load_ray_hw_stack_size_intel:
   case nir_intrinsic_load_ray_sw_stack_size_intel:
   case nir_intrinsic_load_ray_num_dss_rt_stacks_intel:
   case nir_intrinsic_load_ray_hit_sbt_addr_intel:
   case nir_intrinsic_load_ray_hit_sbt_stride_intel:
   case nir_intrinsic_load_ray_miss_sbt_addr_intel:
   case nir_intrinsic_load_ray_miss_sbt_stride_intel:
   case nir_intrinsic_load_callable_sbt_addr_intel:
   case nir_intrinsic_load_callable_sbt_stride_intel:
   case nir_intrinsic_load_reloc_const_intel:
   case nir_intrinsic_load_ray_query_global_intel:
   case nir_intrinsic_load_ray_launch_size:
      /* Notably missing from the above list is btd_local_arg_addr_intel.
   * This is because the resume shader will have a different local
   * argument pointer because it has a different BSR.  Any access of
   * the original shader's local arguments needs to be preserved so
   * that pointer has to be saved on the stack.
   *
   * TODO: There may be some system values we want to avoid
   *       re-materializing as well but we have to be very careful
   *       to ensure that it's a system value which cannot change
   *       across a shader call.
               case nir_intrinsic_resource_intel:
            default:
                     case nir_instr_type_undef:
   case nir_instr_type_load_const:
            default:
            }
      static bool
   can_remat_ssa_def(nir_def *def, struct sized_bitset *remat)
   {
         }
      struct add_instr_data {
      struct util_dynarray *buf;
      };
      static bool
   add_src_instr(nir_src *src, void *state)
   {
      struct add_instr_data *data = state;
   if (BITSET_TEST(data->remat->set, src->ssa->index))
            util_dynarray_foreach(data->buf, nir_instr *, instr_ptr) {
      if (*instr_ptr == src->ssa->parent_instr)
               /* Abort rematerializing an instruction chain if it is too long. */
   if (data->buf->size >= data->buf->capacity)
            util_dynarray_append(data->buf, nir_instr *, src->ssa->parent_instr);
      }
      static int
   compare_instr_indexes(const void *_inst1, const void *_inst2)
   {
      const nir_instr *const *inst1 = _inst1;
               }
      static bool
   can_remat_chain_ssa_def(nir_def *def, struct sized_bitset *remat, struct util_dynarray *buf)
   {
                        /* Add all the instructions involved in build this ssa_def */
            unsigned idx = 0;
   struct add_instr_data data = {
      .buf = buf,
      };
   while (idx < util_dynarray_num_elements(buf, nir_instr *)) {
      nir_instr *instr = *util_dynarray_element(buf, nir_instr *, idx++);
   if (!nir_foreach_src(instr, add_src_instr, &data))
               /* Sort instructions by index */
   qsort(util_dynarray_begin(buf),
         util_dynarray_num_elements(buf, nir_instr *),
            /* Create a temporary bitset with all values already
   * rematerialized/rematerializable. We'll add to this bit set as we go
   * through values that might not be in that set but that we can
   * rematerialize.
   */
   struct sized_bitset potential_remat = bitset_create(mem_ctx, remat->size);
            util_dynarray_foreach(buf, nir_instr *, instr_ptr) {
               /* If already in the potential rematerializable, nothing to do. */
   if (BITSET_TEST(potential_remat.set, instr_ssa_def->index))
            if (!can_remat_instr(*instr_ptr, &potential_remat))
            /* All the sources are rematerializable and the instruction is also
   * rematerializable, mark it as rematerializable too.
   */
                              fail:
      util_dynarray_clear(buf);
   ralloc_free(mem_ctx);
      }
      static nir_def *
   remat_ssa_def(nir_builder *b, nir_def *def, struct hash_table *remap_table)
   {
      nir_instr *clone = nir_instr_clone_deep(b->shader, def->parent_instr, remap_table);
   nir_builder_instr_insert(b, clone);
      }
      static nir_def *
   remat_chain_ssa_def(nir_builder *b, struct util_dynarray *buf,
               {
               util_dynarray_foreach(buf, nir_instr *, instr_ptr) {
      nir_def *instr_ssa_def = nir_instr_def(*instr_ptr);
            if (fill_defs[ssa_index] != NULL &&
                  /* Clone the instruction we want to rematerialize */
            if (fill_defs[ssa_index] == NULL) {
      fill_defs[ssa_index] =
               /* Add the new ssa_def to the list fill_defs and flag it as
   * rematerialized
   */
   fill_defs[ssa_index][call_idx] = last_def = clone_ssa_def;
                           }
      struct pbv_array {
      struct nir_phi_builder_value **arr;
      };
      static struct nir_phi_builder_value *
   get_phi_builder_value_for_def(nir_def *def,
         {
      if (def->index >= pbv_arr->len)
               }
      static nir_def *
   get_phi_builder_def_for_src(nir_src *src, struct pbv_array *pbv_arr,
         {
         struct nir_phi_builder_value *pbv =
         if (pbv == NULL)
               }
      static bool
   rewrite_instr_src_from_phi_builder(nir_src *src, void *_pbv_arr)
   {
      nir_block *block;
   if (nir_src_parent_instr(src)->type == nir_instr_type_phi) {
      nir_phi_src *phi_src = exec_node_data(nir_phi_src, src, src);
      } else {
                  nir_def *new_def = get_phi_builder_def_for_src(src, _pbv_arr, block);
   if (new_def != NULL)
            }
      static nir_def *
   spill_fill(nir_builder *before, nir_builder *after, nir_def *def,
               {
               nir_store_stack(before, def,
                  .base = offset,
   .call_idx = call_idx,
      return nir_load_stack(after, def->num_components, def->bit_size,
                        }
      static bool
   add_src_to_call_live_bitset(nir_src *src, void *state)
   {
               BITSET_SET(call_live, src->ssa->index);
      }
      static void
   spill_ssa_defs_and_lower_shader_calls(nir_shader *shader, uint32_t num_calls,
         {
      /* TODO: If a SSA def is filled more than once, we probably want to just
   *       spill it at the LCM of the fill sites so we avoid unnecessary
   *       extra spills
   *
   * TODO: If a SSA def is defined outside a loop but live through some call
   *       inside the loop, we probably want to spill outside the loop.  We
   *       may also want to fill outside the loop if it's not used in the
   *       loop.
   *
   * TODO: Right now, we only re-materialize things if their immediate
   *       sources are things which we filled.  We probably want to expand
   *       that to re-materialize things whose sources are things we can
   *       re-materialize from things we filled.  We may want some DAG depth
   *       heuristic on this.
            /* This happens per-shader rather than per-impl because we mess with
   * nir_shader::scratch_size.
   */
            nir_metadata_require(impl, nir_metadata_live_defs |
                                 const unsigned num_ssa_defs = impl->ssa_alloc;
   const unsigned live_words = BITSET_WORDS(num_ssa_defs);
            /* Array of all live SSA defs which are spill candidates */
   nir_def **spill_defs =
            /* For each spill candidate, an array of every time it's defined by a fill,
   * indexed by call instruction index.
   */
   nir_def ***fill_defs =
            /* For each call instruction, the liveness set at the call */
   const BITSET_WORD **call_live =
            /* For each call instruction, the block index of the block it lives in */
            /* Remap table when rebuilding instructions out of fill operations */
   struct hash_table *trivial_remap_table =
            /* Walk the call instructions and fetch the liveness set and block index
   * for each one.  We need to do this before we start modifying the shader
   * so that liveness doesn't complain that it's been invalidated.  Don't
   * worry, we'll be very careful with our live sets. :-)
   */
   unsigned call_idx = 0;
   nir_foreach_block(block, impl) {
      nir_foreach_instr(instr, block) {
                              /* The objective here is to preserve values around shader call
   * instructions.  Therefore, we use the live set after the
   * instruction as the set of things we want to preserve.  Because
   * none of our shader call intrinsics return anything, we don't have
   * to worry about spilling over a return value.
   *
   * TODO: This isn't quite true for report_intersection.
   */
                                 /* If a should_remat_callback is given, call it on each of the live values
   * for each call site. If it returns true we need to rematerialize that
   * instruction (instead of spill/fill). Therefore we need to add the
   * sources as live values so that we can rematerialize on top of those
   * spilled/filled sources.
   */
   if (options->should_remat_callback) {
      BITSET_WORD **updated_call_live =
            nir_foreach_block(block, impl) {
      nir_foreach_instr(instr, block) {
      nir_def *def = nir_instr_def(instr);
                  for (unsigned c = 0; c < num_calls; c++) {
                                          if (updated_call_live[c] == NULL) {
      const unsigned bitset_words = BITSET_WORDS(impl->ssa_alloc);
                                       for (unsigned c = 0; c < num_calls; c++) {
      if (updated_call_live[c] != NULL)
                  nir_builder before, after;
   before = nir_builder_create(impl);
            call_idx = 0;
   unsigned max_scratch_size = shader->scratch_size;
   nir_foreach_block(block, impl) {
      nir_foreach_instr_safe(instr, block) {
      nir_def *def = nir_instr_def(instr);
   if (def != NULL) {
      if (can_remat_ssa_def(def, &trivial_remat)) {
      add_ssa_def_to_bitset(def, &trivial_remat);
      } else {
                                                            /* Make a copy of trivial_remat that we'll update as we crawl through
   * the live SSA defs and unspill them.
   */
                  /* Before the two builders are always separated by the call
   * instruction, it won't break anything to have two of them.
   */
                  /* Array used to hold all the values needed to rematerialize a live
   * value. The capacity is used to determine when we should abort testing
   * a remat chain. In practice, shaders can have chains with more than
   * 10k elements while only chains with less than 16 have realistic
   * chances. There also isn't any performance benefit in rematerializing
   * extremely long chains.
   */
   nir_instr *remat_chain_instrs[16];
                  unsigned offset = shader->scratch_size;
   for (unsigned w = 0; w < live_words; w++) {
      BITSET_WORD spill_mask = live[w] & ~trivial_remat.set[w];
   while (spill_mask) {
      int i = u_bit_scan(&spill_mask);
                        def = spill_defs[index];
   nir_def *original_def = def, *new_def;
   if (can_remat_ssa_def(def, &remat)) {
      /* If this SSA def is re-materializable or based on other
   * things we've already spilled, re-materialize it rather
   * than spilling and filling.  Anything which is trivially
   * re-materializable won't even get here because we take
   * those into account in spill_mask above.
   */
      } else if (can_remat_chain_ssa_def(def, &remat, &remat_chain)) {
      new_def = remat_chain_ssa_def(&after, &remat_chain, &remat,
                  } else {
                                                                                          /* Mark this SSA def as available in the remat set so that, if
   * some other SSA def we need is computed based on it, we can
                        /* For now, we just make a note of this new SSA def.  We'll
   * fix things up with the phi builder as a second pass.
   */
   if (fill_defs[index] == NULL) {
      fill_defs[index] =
      }
   fill_defs[index][call_idx] = new_def;
                                          /* First thing on the called shader's stack is the resume address
   * followed by a pointer to the payload.
                  /* Lower to generic intrinsics with information about the stack & resume shader. */
   switch (call->intrinsic) {
   case nir_intrinsic_trace_ray: {
      nir_rt_trace_ray(b, call->src[0].ssa, call->src[1].ssa,
                  call->src[2].ssa, call->src[3].ssa,
   call->src[4].ssa, call->src[5].ssa,
   call->src[6].ssa, call->src[7].ssa,
                                 case nir_intrinsic_execute_callable: {
      nir_rt_execute_callable(b, call->src[0].ssa, call->src[1].ssa, .call_idx = call_idx, .stack_size = offset);
               default:
                                          }
   assert(call_idx == num_calls);
            struct nir_phi_builder *pb = nir_phi_builder_create(impl);
   struct pbv_array pbv_arr = {
      .arr = rzalloc_array(mem_ctx, struct nir_phi_builder_value *,
                     const unsigned block_words = BITSET_WORDS(impl->num_blocks);
            /* Go through and set up phi builder values for each spillable value which
   * we ever needed to spill at any point.
   */
   for (unsigned index = 0; index < num_ssa_defs; index++) {
      if (fill_defs[index] == NULL)
                     memset(def_blocks, 0, block_words * sizeof(BITSET_WORD));
   BITSET_SET(def_blocks, def->parent_instr->block->index);
   for (unsigned call_idx = 0; call_idx < num_calls; call_idx++) {
      if (fill_defs[index][call_idx] != NULL)
               pbv_arr.arr[index] = nir_phi_builder_add_value(pb, def->num_components,
               /* Walk the shader one more time and rewrite SSA defs as needed using the
   * phi builder.
   */
   nir_foreach_block(block, impl) {
      nir_foreach_instr_safe(instr, block) {
      nir_def *def = nir_instr_def(instr);
   if (def != NULL) {
      struct nir_phi_builder_value *pbv =
         if (pbv != NULL)
                                                      nir_intrinsic_instr *resume = nir_instr_as_intrinsic(instr);
                           /* Technically, this is the wrong place to add the fill defs to the
   * phi builder values because we haven't seen any of the load_scratch
   * instructions for this call yet.  However, we know based on how we
   * emitted them that no value ever gets used until after the load
   * instruction has been emitted so this should be safe.  If we ever
   * fail validation due this it likely means a bug in our spilling
   * code and not the phi re-construction code here.
   */
   for (unsigned index = 0; index < num_ssa_defs; index++) {
      if (fill_defs[index] && fill_defs[index][call_idx]) {
      nir_phi_builder_value_set_block_def(pbv_arr.arr[index], block,
                     nir_if *following_if = nir_block_get_following_if(block);
   if (following_if) {
      nir_def *new_def =
      get_phi_builder_def_for_src(&following_if->condition,
      if (new_def != NULL)
               /* Handle phi sources that source from this block.  We have to do this
   * as a separate pass because the phi builder assumes that uses and
   * defs are processed in an order that respects dominance.  When we have
   * loops, a phi source may be a back-edge so we have to handle it as if
   * it were one of the last instructions in the predecessor block.
   */
   nir_foreach_phi_src_leaving_block(block,
                                       nir_metadata_preserve(impl, nir_metadata_block_index |
      }
      static nir_instr *
   find_resume_instr(nir_function_impl *impl, unsigned call_idx)
   {
      nir_foreach_block(block, impl) {
      nir_foreach_instr(instr, block) {
                     nir_intrinsic_instr *resume = nir_instr_as_intrinsic(instr);
                  if (nir_intrinsic_call_idx(resume) == call_idx)
         }
      }
      /* Walk the CF tree and duplicate the contents of every loop, one half runs on
   * resume and the other half is for any post-resume loop iterations.  We are
   * careful in our duplication to ensure that resume_instr is in the resume
   * half of the loop though a copy of resume_instr will remain in the other
   * half as well in case the same shader call happens twice.
   */
   static bool
   duplicate_loop_bodies(nir_function_impl *impl, nir_instr *resume_instr)
   {
      nir_def *resume_reg = NULL;
   for (nir_cf_node *node = resume_instr->block->cf_node.parent;
      node->type != nir_cf_node_function; node = node->parent) {
   if (node->type != nir_cf_node_loop)
            nir_loop *loop = nir_cf_node_as_loop(node);
                     if (resume_reg == NULL) {
      /* We only create resume_reg if we encounter a loop.  This way we can
   * avoid re-validating the shader and calling ssa_to_reg_intrinsics in
   * the case where it's just if-ladders.
                  /* Initialize resume to true at the start of the shader, right after
   * the register is declared at the start.
   */
                  /* Set resume to false right after the resume instruction */
   b.cursor = nir_after_instr(resume_instr);
               /* Before we go any further, make sure that everything which exits the
   * loop or continues around to the top of the loop does so through
   * registers.  We're about to duplicate the loop body and we'll have
   * serious trouble if we don't do this.
   */
   nir_convert_loop_to_lcssa(loop);
   nir_lower_phis_to_regs_block(nir_loop_first_block(loop));
   nir_lower_phis_to_regs_block(
            nir_cf_list cf_list;
            nir_if *_if = nir_if_create(impl->function->shader);
   b.cursor = nir_after_cf_list(&loop->body);
   _if->condition = nir_src_for_ssa(nir_load_reg(&b, resume_reg));
            nir_cf_list clone;
            /* Insert the clone in the else and the original in the then so that
   * the resume_instr remains valid even after the duplication.
   */
   nir_cf_reinsert(&cf_list, nir_before_cf_list(&_if->then_list));
               if (resume_reg != NULL)
               }
      static bool
   cf_node_contains_block(nir_cf_node *node, nir_block *block)
   {
      for (nir_cf_node *n = &block->cf_node; n != NULL; n = n->parent) {
      if (n == node)
                  }
      static void
   rewrite_phis_to_pred(nir_block *block, nir_block *pred)
   {
      nir_foreach_phi(phi, block) {
      ASSERTED bool found = false;
   nir_foreach_phi_src(phi_src, phi) {
      if (phi_src->pred == pred) {
      found = true;
   nir_def_rewrite_uses(&phi->def, phi_src->src.ssa);
         }
         }
      static bool
   cursor_is_after_jump(nir_cursor cursor)
   {
      switch (cursor.option) {
   case nir_cursor_before_instr:
   case nir_cursor_before_block:
         case nir_cursor_after_instr:
         case nir_cursor_after_block:
      return nir_block_ends_in_jump(cursor.block);
      }
      }
      /** Flattens if ladders leading up to a resume
   *
   * Given a resume_instr, this function flattens any if ladders leading to the
   * resume instruction and deletes any code that cannot be encountered on a
   * direct path to the resume instruction.  This way we get, for the most part,
   * straight-line control-flow up to the resume instruction.
   *
   * While we do this flattening, we also move any code which is in the remat
   * set up to the top of the function or to the top of the resume portion of
   * the current loop.  We don't worry about control-flow as we do this because
   * phis will never be in the remat set (see can_remat_instr) and so nothing
   * control-dependent will ever need to be re-materialized.  It is possible
   * that this algorithm will preserve too many instructions by moving them to
   * the top but we leave that for DCE to clean up.  Any code not in the remat
   * set is deleted because it's either unused in the continuation or else
   * unspilled from a previous continuation and the unspill code is after the
   * resume instruction.
   *
   * If, for instance, we have something like this:
   *
   *    // block 0
   *    if (cond1) {
   *       // block 1
   *    } else {
   *       // block 2
   *       if (cond2) {
   *          // block 3
   *          resume;
   *          if (cond3) {
   *             // block 4
   *          }
   *       } else {
   *          // block 5
   *       }
   *    }
   *
   * then we know, because we know the resume instruction had to be encoutered,
   * that cond1 = false and cond2 = true and we lower as follows:
   *
   *    // block 0
   *    // block 2
   *    // block 3
   *    resume;
   *    if (cond3) {
   *       // block 4
   *    }
   *
   * As you can see, the code in blocks 1 and 5 was removed because there is no
   * path from the start of the shader to the resume instruction which execute
   * blocks 1 or 5.  Any remat code from blocks 0, 2, and 3 is preserved and
   * moved to the top.  If the resume instruction is inside a loop then we know
   * a priori that it is of the form
   *
   *    loop {
   *       if (resume) {
   *          // Contents containing resume_instr
   *       } else {
   *          // Second copy of contents
   *       }
   *    }
   *
   * In this case, we only descend into the first half of the loop.  The second
   * half is left alone as that portion is only ever executed after the resume
   * instruction.
   */
   static bool
   flatten_resume_if_ladder(nir_builder *b,
                           nir_cf_node *parent_node,
   {
               /* If our child list contains the cursor instruction then we start out
   * before the cursor instruction.  We need to know this so that we can skip
   * moving instructions which are already before the cursor.
   */
            nir_cf_node *resume_node = NULL;
   foreach_list_typed_safe(nir_cf_node, child, node, child_list) {
      switch (child->type) {
   case nir_cf_node_block: {
      nir_block *block = nir_cf_node_as_block(child);
   if (b->cursor.option == nir_cursor_before_block &&
      b->cursor.block == block) {
   assert(before_cursor);
      }
   nir_foreach_instr_safe(instr, block) {
      if ((b->cursor.option == nir_cursor_before_instr ||
      b->cursor.option == nir_cursor_after_instr) &&
   b->cursor.instr == instr) {
   assert(nir_cf_node_is_first(&block->cf_node));
   assert(before_cursor);
                                    if (!before_cursor && can_remat_instr(instr, remat)) {
                           nir_def *def = nir_instr_def(instr);
         }
   if (b->cursor.option == nir_cursor_after_block &&
      b->cursor.block == block) {
   assert(before_cursor);
      }
               case nir_cf_node_if: {
      assert(!before_cursor);
   nir_if *_if = nir_cf_node_as_if(child);
   if (flatten_resume_if_ladder(b, &_if->cf_node, &_if->then_list,
            resume_node = child;
   rewrite_phis_to_pred(nir_cf_node_as_block(nir_cf_node_next(child)),
                     if (flatten_resume_if_ladder(b, &_if->cf_node, &_if->else_list,
            resume_node = child;
   rewrite_phis_to_pred(nir_cf_node_as_block(nir_cf_node_next(child)),
            }
               case nir_cf_node_loop: {
      assert(!before_cursor);
                  if (cf_node_contains_block(&loop->cf_node, resume_instr->block)) {
      /* Thanks to our loop body duplication pass, every level of loop
   * containing the resume instruction contains exactly three nodes:
   * two blocks and an if.  We don't want to lower away this if
   * because it's the resume selection if.  The resume half is
   * always the then_list so that's what we want to flatten.
   */
                  /* We want to place anything re-materialized from inside the loop
   * at the top of the resume half of the loop.
                  ASSERTED bool found =
      flatten_resume_if_ladder(&bl, &_if->cf_node, &_if->then_list,
      assert(found);
   resume_node = child;
      } else {
      ASSERTED bool found =
      flatten_resume_if_ladder(b, &loop->cf_node, &loop->body,
         }
               case nir_cf_node_function:
            }
            /* If we got here, we didn't find the resume node or instruction. */
         found_resume:
      /* If we got here then we found either the resume node or the resume
   * instruction in this CF list.
   */
   if (resume_node) {
      /* If the resume instruction is buried in side one of our children CF
   * nodes, resume_node now points to that child.
   */
   if (resume_node->type == nir_cf_node_if) {
      /* Thanks to the recursive call, all of the interesting contents of
   * resume_node have been copied before the cursor.  We just need to
   * copy the stuff after resume_node.
   */
   nir_cf_extract(&cf_list, nir_after_cf_node(resume_node),
      } else {
      /* The loop contains its own cursor and still has useful stuff in it.
   * We want to move everything after and including the loop to before
   * the cursor.
   */
   assert(resume_node->type == nir_cf_node_loop);
   nir_cf_extract(&cf_list, nir_before_cf_node(resume_node),
         } else {
      /* If we found the resume instruction in one of our blocks, grab
   * everything after it in the entire list (not just the one block), and
   * place it before the cursor instr.
   */
   nir_cf_extract(&cf_list, nir_after_instr(resume_instr),
               /* If the resume instruction is in the first block of the child_list,
   * and the cursor is still before that block, the nir_cf_extract() may
   * extract the block object pointed by the cursor, and instead create
   * a new one for the code before the resume. In such case the cursor
   * will be broken, as it will point to a block which is no longer
   * in a function.
   *
   * Luckily, in both cases when this is possible, the intended cursor
   * position is right before the child_list, so we can fix the cursor here.
   */
   if (child_list_contains_cursor &&
      b->cursor.option == nir_cursor_before_block &&
   b->cursor.block->cf_node.parent == NULL)
         if (cursor_is_after_jump(b->cursor)) {
      /* If the resume instruction is in a loop, it's possible cf_list ends
   * in a break or continue instruction, in which case we don't want to
   * insert anything.  It's also possible we have an early return if
   * someone hasn't lowered those yet.  In either case, nothing after that
   * point executes in this context so we can delete it.
   */
      } else {
                  if (!resume_node) {
      /* We want the resume to be the first "interesting" instruction */
   nir_instr_remove(resume_instr);
               /* We've copied everything interesting out of this CF list to before the
   * cursor.  Delete everything else.
   */
   if (child_list_contains_cursor) {
         } else {
         }
               }
      typedef bool (*wrap_instr_callback)(nir_instr *instr);
      static bool
   wrap_instr(nir_builder *b, nir_instr *instr, void *data)
   {
      wrap_instr_callback callback = data;
   if (!callback(instr))
                     nir_if *_if = nir_push_if(b, nir_imm_true(b));
            nir_cf_list cf_list;
   nir_cf_extract(&cf_list, nir_before_instr(instr), nir_after_instr(instr));
               }
      /* This pass wraps jump instructions in a dummy if block so that when
   * flatten_resume_if_ladder() does its job, it doesn't move a jump instruction
   * directly in front of another instruction which the NIR control flow helpers
   * do not allow.
   */
   static bool
   wrap_instrs(nir_shader *shader, wrap_instr_callback callback)
   {
      return nir_shader_instructions_pass(shader, wrap_instr,
      }
      static bool
   instr_is_jump(nir_instr *instr)
   {
         }
      static nir_instr *
   lower_resume(nir_shader *shader, int call_idx)
   {
               nir_function_impl *impl = nir_shader_get_entrypoint(shader);
            if (duplicate_loop_bodies(impl, resume_instr)) {
      nir_validate_shader(shader, "after duplicate_loop_bodies in "
         /* If we duplicated the bodies of any loops, run reg_intrinsics_to_ssa to
   * get rid of all those pesky registers we just added.
   */
               /* Re-index nir_def::index.  We don't care about actual liveness in
   * this pass but, so we can use the same helpers as the spilling pass, we
   * need to make sure that live_index is something sane.  It's used
   * constantly for determining if an SSA value has been added since the
   * start of the pass.
   */
                     /* Used to track which things may have been assumed to be re-materialized
   * by the spilling pass and which we shouldn't delete.
   */
            /* Create a nop instruction to use as a cursor as we extract and re-insert
   * stuff into the CFG.
   */
   nir_builder b = nir_builder_at(nir_before_impl(impl));
   ASSERTED bool found =
      flatten_resume_if_ladder(&b, &impl->cf_node, &impl->body,
                                 nir_validate_shader(shader, "after flatten_resume_if_ladder in "
               }
      static void
   replace_resume_with_halt(nir_shader *shader, nir_instr *keep)
   {
                        nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                                    nir_intrinsic_instr *resume = nir_instr_as_intrinsic(instr);
                  /* If this is some other resume, then we've kicked off a ray or
   * bindless thread and we don't want to go any further in this
   * shader.  Insert a halt so that NIR will delete any instructions
   * dominated by this call instruction including the scratch_load
   * instructions we inserted.
   */
   nir_cf_list cf_list;
   nir_cf_extract(&cf_list, nir_after_instr(&resume->instr),
         nir_cf_delete(&cf_list);
   b.cursor = nir_instr_remove(&resume->instr);
   nir_jump(&b, nir_jump_halt);
            }
      struct lower_scratch_state {
         };
      static bool
   lower_stack_instr_to_scratch(struct nir_builder *b, nir_instr *instr, void *data)
   {
               if (instr->type != nir_instr_type_intrinsic)
            nir_intrinsic_instr *stack = nir_instr_as_intrinsic(instr);
   switch (stack->intrinsic) {
   case nir_intrinsic_load_stack: {
      b->cursor = nir_instr_remove(instr);
            if (state->address_format == nir_address_format_64bit_global) {
      nir_def *addr = nir_iadd_imm(b,
               data = nir_build_load_global(b,
                              } else {
      assert(state->address_format == nir_address_format_32bit_offset);
   data = nir_load_scratch(b,
                              }
   nir_def_rewrite_uses(old_data, data);
               case nir_intrinsic_store_stack: {
      b->cursor = nir_instr_remove(instr);
            if (state->address_format == nir_address_format_64bit_global) {
      nir_def *addr = nir_iadd_imm(b,
               nir_store_global(b, addr,
                  } else {
      assert(state->address_format == nir_address_format_32bit_offset);
   nir_store_scratch(b, data,
                  }
               default:
                     }
      static bool
   nir_lower_stack_to_scratch(nir_shader *shader,
         {
      struct lower_scratch_state state = {
                  return nir_shader_instructions_pass(shader,
                        }
      static bool
   opt_remove_respills_instr(struct nir_builder *b,
         {
      if (store_intrin->intrinsic != nir_intrinsic_store_stack)
            nir_instr *value_instr = store_intrin->src[0].ssa->parent_instr;
   if (value_instr->type != nir_instr_type_intrinsic)
            nir_intrinsic_instr *load_intrin = nir_instr_as_intrinsic(value_instr);
   if (load_intrin->intrinsic != nir_intrinsic_load_stack)
            if (nir_intrinsic_base(load_intrin) != nir_intrinsic_base(store_intrin))
            nir_instr_remove(&store_intrin->instr);
      }
      /* After shader split, look at stack load/store operations. If we're loading
   * and storing the same value at the same location, we can drop the store
   * instruction.
   */
   static bool
   nir_opt_remove_respills(nir_shader *shader)
   {
      return nir_shader_intrinsics_pass(shader, opt_remove_respills_instr,
                  }
      static void
   add_use_mask(struct hash_table_u64 *offset_to_mask,
         {
      uintptr_t old_mask = (uintptr_t)
            _mesa_hash_table_u64_insert(offset_to_mask, offset,
      }
      /* When splitting the shaders, we might have inserted store & loads of vec4s,
   * because a live value is a 4 components. But sometimes, only some components
   * of that vec4 will be used by after the scratch load. This pass removes the
   * unused components of scratch load/stores.
   */
   static bool
   nir_opt_trim_stack_values(nir_shader *shader)
   {
               struct hash_table_u64 *value_id_to_mask = _mesa_hash_table_u64_create(NULL);
            /* Find all the loads and how their value is being used */
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
                           const unsigned mask =
                        /* For each store, if it stores more than is being used, trim it.
   * Otherwise, remove it from the hash table.
   */
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
                           const unsigned write_mask = nir_intrinsic_write_mask(intrin);
                  /* Already removed from the table, nothing to do */
                  /* Matching read/write mask, nothing to do, remove from the table. */
   if (write_mask == read_mask) {
      _mesa_hash_table_u64_remove(value_id_to_mask, value_id);
                                                                     /* For each load remaining in the hash table (only the ones we changed the
   * number of components of), apply triming/reswizzle.
   */
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
                           unsigned read_mask = (uintptr_t)
                        unsigned swiz_map[NIR_MAX_VEC_COMPONENTS] = {
         };
   unsigned swiz_count = 0;
                           nir_foreach_use_safe(use_src, def) {
      if (nir_src_parent_instr(use_src)->type == nir_instr_type_alu) {
                     unsigned count = alu->def.num_components;
   for (unsigned idx = 0; idx < count; ++idx)
      } else if (nir_src_parent_instr(use_src)->type == nir_instr_type_intrinsic) {
      nir_intrinsic_instr *use_intrin =
         assert(nir_intrinsic_has_write_mask(use_intrin));
   unsigned write_mask = nir_intrinsic_write_mask(use_intrin);
   unsigned new_write_mask = 0;
   u_foreach_bit(idx, write_mask)
            } else {
                                             nir_metadata_preserve(impl,
                                          }
      struct scratch_item {
      unsigned old_offset;
   unsigned new_offset;
   unsigned bit_size;
   unsigned num_components;
   unsigned value;
      };
      static int
   sort_scratch_item_by_size_and_value_id(const void *_item1, const void *_item2)
   {
      const struct scratch_item *item1 = _item1;
            /* By ascending value_id */
   if (item1->bit_size == item2->bit_size)
            /* By descending size */
      }
      static bool
   nir_opt_sort_and_pack_stack(nir_shader *shader,
                     {
                        struct hash_table_u64 *value_id_to_item =
         struct util_dynarray ops;
            for (unsigned call_idx = 0; call_idx < num_calls; call_idx++) {
      _mesa_hash_table_u64_clear(value_id_to_item);
            /* Find all the stack load and their offset. */
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
                                                               struct scratch_item item = {
      .old_offset = nir_intrinsic_base(intrin),
   .bit_size = def->bit_size,
                     util_dynarray_append(&ops, struct scratch_item, item);
                  /* Sort scratch item by component size. */
   if (util_dynarray_num_elements(&ops, struct scratch_item)) {
      qsort(util_dynarray_begin(&ops),
         util_dynarray_num_elements(&ops, struct scratch_item),
               /* Reorder things on the stack */
            unsigned scratch_size = start_call_scratch;
   util_dynarray_foreach(&ops, struct scratch_item, item) {
      item->new_offset = ALIGN(scratch_size, item->bit_size / 8);
   scratch_size = item->new_offset + (item->bit_size * item->num_components) / 8;
      }
            /* Update offsets in the instructions */
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
   switch (intrin->intrinsic) {
   case nir_intrinsic_load_stack:
   case nir_intrinsic_store_stack: {
                     struct scratch_item *item =
                                          case nir_intrinsic_rt_trace_ray:
   case nir_intrinsic_rt_execute_callable:
   case nir_intrinsic_rt_resume:
      if (nir_intrinsic_call_idx(intrin) != call_idx)
                     default:
                                                }
      static unsigned
   nir_block_loop_depth(nir_block *block)
   {
      nir_cf_node *node = &block->cf_node;
            while (node != NULL) {
      if (node->type == nir_cf_node_loop)
                        }
      /* Find the last block dominating all the uses of a SSA value. */
   static nir_block *
   find_last_dominant_use_block(nir_function_impl *impl, nir_def *value)
   {
      nir_block *old_block = value->parent_instr->block;
            nir_foreach_block_reverse_safe(block, impl) {
               /* Store on the current block of the value */
   if (block == old_block)
            /* Don't move instructions deeper into loops, this would generate more
   * memory traffic.
   */
   unsigned block_loop_depth = nir_block_loop_depth(block);
   if (block_loop_depth > old_block_loop_depth)
            nir_foreach_if_use(src, value) {
      nir_block *block_before_if =
         if (!nir_block_dominates(block, block_before_if)) {
      fits = false;
         }
   if (!fits)
            nir_foreach_use(src, value) {
      if (nir_src_parent_instr(src)->type == nir_instr_type_phi &&
      block == nir_src_parent_instr(src)->block) {
   fits = false;
               if (!nir_block_dominates(block, nir_src_parent_instr(src)->block)) {
      fits = false;
         }
   if (!fits)
               }
      }
      /* Put the scratch loads in the branches where they're needed. */
   static bool
   nir_opt_stack_loads(nir_shader *shader)
   {
               nir_foreach_function_impl(impl, shader) {
      nir_metadata_require(impl, nir_metadata_dominance |
            bool func_progress = false;
   nir_foreach_block_safe(block, impl) {
      nir_foreach_instr_safe(instr, block) {
                     nir_intrinsic_instr *intrin = nir_instr_as_intrinsic(instr);
                  nir_def *value = &intrin->def;
   nir_block *new_block = find_last_dominant_use_block(impl, value);
                  /* Move the scratch load in the new block, after the phis. */
                                 nir_metadata_preserve(impl,
                                             }
      static bool
   split_stack_components_instr(struct nir_builder *b,
         {
      if (intrin->intrinsic != nir_intrinsic_load_stack &&
      intrin->intrinsic != nir_intrinsic_store_stack)
         if (intrin->intrinsic == nir_intrinsic_load_stack &&
      intrin->def.num_components == 1)
         if (intrin->intrinsic == nir_intrinsic_store_stack &&
      intrin->src[0].ssa->num_components == 1)
                  unsigned align_mul = nir_intrinsic_align_mul(intrin);
   unsigned align_offset = nir_intrinsic_align_offset(intrin);
   if (intrin->intrinsic == nir_intrinsic_load_stack) {
      nir_def *components[NIR_MAX_VEC_COMPONENTS] = {
         };
   for (unsigned c = 0; c < intrin->def.num_components; c++) {
      unsigned offset = c * intrin->def.bit_size / 8;
   components[c] = nir_load_stack(b, 1, intrin->def.bit_size,
                                       nir_def_rewrite_uses(&intrin->def,
            } else {
      assert(intrin->intrinsic == nir_intrinsic_store_stack);
   for (unsigned c = 0; c < intrin->src[0].ssa->num_components; c++) {
      unsigned offset = c * intrin->src[0].ssa->bit_size / 8;
   nir_store_stack(b, nir_channel(b, intrin->src[0].ssa, c),
                  .base = nir_intrinsic_base(intrin) + offset,
   .call_idx = nir_intrinsic_call_idx(intrin),
   .align_mul = align_mul,
                           }
      /* Break the load_stack/store_stack intrinsics into single compoments. This
   * helps the vectorizer to pack components.
   */
   static bool
   nir_split_stack_components(nir_shader *shader)
   {
      return nir_shader_intrinsics_pass(shader, split_stack_components_instr,
                  }
      struct stack_op_vectorizer_state {
      nir_should_vectorize_mem_func driver_callback;
      };
      static bool
   should_vectorize(unsigned align_mul,
                  unsigned align_offset,
   unsigned bit_size,
      {
      /* We only care about those intrinsics */
   if ((low->intrinsic != nir_intrinsic_load_stack &&
      low->intrinsic != nir_intrinsic_store_stack) ||
   (high->intrinsic != nir_intrinsic_load_stack &&
   high->intrinsic != nir_intrinsic_store_stack))
                  return state->driver_callback(align_mul, align_offset,
            }
      /** Lower shader call instructions to split shaders.
   *
   * Shader calls can be split into an initial shader and a series of "resume"
   * shaders.   When the shader is first invoked, it is the initial shader which
   * is executed.  At any point in the initial shader or any one of the resume
   * shaders, a shader call operation may be performed.  The possible shader call
   * operations are:
   *
   *  - trace_ray
   *  - report_ray_intersection
   *  - execute_callable
   *
   * When a shader call operation is performed, we push all live values to the
   * stack,call rt_trace_ray/rt_execute_callable and then kill the shader. Once
   * the operation we invoked is complete, a callee shader will return execution
   * to the respective resume shader. The resume shader pops the contents off
   * the stack and picks up where the calling shader left off.
   *
   * Stack management is assumed to be done after this pass. Call
   * instructions and their resumes get annotated with stack information that
   * should be enough for the backend to implement proper stack management.
   */
   bool
   nir_lower_shader_calls(nir_shader *shader,
                           {
               int num_calls = 0;
   nir_foreach_block(block, impl) {
      nir_foreach_instr_safe(instr, block) {
      if (instr_is_shader_call(instr))
                  if (num_calls == 0) {
      nir_shader_preserve_all_metadata(shader);
   *num_resume_shaders_out = 0;
               /* Some intrinsics not only can't be re-materialized but aren't preserved
   * when moving to the continuation shader.  We have to move them to the top
   * to ensure they get spilled as needed.
   */
   {
      bool progress = false;
   NIR_PASS(progress, shader, move_system_values_to_top);
   if (progress)
               /* Deref chains contain metadata information that is needed by other passes
   * after this one. If we don't rematerialize the derefs in the blocks where
   * they're used here, the following lowerings will insert phis which can
   * prevent other passes from chasing deref chains. Additionally, derefs need
   * to be rematerialized after shader call instructions to avoid spilling.
   */
   {
      bool progress = false;
                     if (progress)
               /* Save the start point of the call stack in scratch */
            NIR_PASS_V(shader, spill_ssa_defs_and_lower_shader_calls,
                     NIR_PASS_V(shader, nir_opt_trim_stack_values);
   NIR_PASS_V(shader, nir_opt_sort_and_pack_stack,
            /* Make N copies of our shader */
   nir_shader **resume_shaders = ralloc_array(mem_ctx, nir_shader *, num_calls);
   for (unsigned i = 0; i < num_calls; i++) {
               /* Give them a recognizable name */
   resume_shaders[i]->info.name =
      ralloc_asprintf(mem_ctx, "%s%sresume_%u",
                        replace_resume_with_halt(shader, NULL);
   nir_opt_dce(shader);
   nir_opt_dead_cf(shader);
   for (unsigned i = 0; i < num_calls; i++) {
      nir_instr *resume_instr = lower_resume(resume_shaders[i], i);
   replace_resume_with_halt(resume_shaders[i], resume_instr);
   /* Remove CF after halt before nir_opt_if(). */
   nir_opt_dead_cf(resume_shaders[i]);
   /* Remove the dummy blocks added by flatten_resume_if_ladder() */
   nir_opt_if(resume_shaders[i], nir_opt_if_optimize_phi_true_false);
   nir_opt_dce(resume_shaders[i]);
   nir_opt_dead_cf(resume_shaders[i]);
               for (unsigned i = 0; i < num_calls; i++)
            if (options->localized_loads) {
      /* Once loads have been combined we can try to put them closer to where
   * they're needed.
   */
   for (unsigned i = 0; i < num_calls; i++)
               struct stack_op_vectorizer_state vectorizer_state = {
      .driver_callback = options->vectorizer_callback,
      };
   nir_load_store_vectorize_options vect_opts = {
      .modes = nir_var_shader_temp,
   .callback = should_vectorize,
               if (options->vectorizer_callback != NULL) {
      NIR_PASS_V(shader, nir_split_stack_components);
      }
   NIR_PASS_V(shader, nir_lower_stack_to_scratch, options->address_format);
   nir_opt_cse(shader);
   for (unsigned i = 0; i < num_calls; i++) {
      if (options->vectorizer_callback != NULL) {
      NIR_PASS_V(resume_shaders[i], nir_split_stack_components);
      }
   NIR_PASS_V(resume_shaders[i], nir_lower_stack_to_scratch,
                     *resume_shaders_out = resume_shaders;
               }
