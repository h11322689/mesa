   /*
   * Copyright Â© 2023 Bas Nieuwenhuizen
   *
   * Permission is hereby granted, free of charge, to any person obtaining a
   * copy of this software and associated documentation files (the "Software"),
   * to deal in the Software without restriction, including without limitation
   * the rights to use, copy, modify, merge, publish, distribute, sublicense,
   * and/or sell copies of the Software, and to permit persons to whom the
   * Software is furnished to do so, subject to the following conditions:
   *
   * The above copyright notice and this permission notice (including the next
   * paragraph) shall be included in all copies or substantial portions of the
   * Software.
   *
   * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
   * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
   * IN THE SOFTWARE.
   */
      #include "nir_builder.h"
   #include "radv_nir.h"
      static unsigned
   radv_nir_cmat_length(struct glsl_cmat_description desc, unsigned wave_size)
   {
      return desc.use != GLSL_CMAT_USE_ACCUMULATOR
            }
      /* for C matrices we have 1 VGPR per element even if the element type is < 32 bits. So with 8 fp16 elements we implement
   * that with a f16vec16. We then use the coefficient generated by this function to figure out how many elements we
   * really have.
   */
   static unsigned
   radv_nir_cmat_length_mul(struct glsl_cmat_description desc)
   {
         }
      static unsigned
   radv_nir_cmat_bits(struct glsl_cmat_description desc)
   {
         }
      static nir_def *
   radv_nir_load_cmat(nir_builder *b, unsigned wave_size, nir_def *src)
   {
      nir_deref_instr *deref = nir_instr_as_deref(src->parent_instr);
   struct glsl_cmat_description desc = *glsl_get_cmat_description(deref->type);
   return nir_build_load_deref(b, radv_nir_cmat_length(desc, wave_size), glsl_base_type_bit_size(desc.element_type),
      }
      static const struct glsl_type *
   radv_nir_translate_matrix_type(const struct glsl_type *orig_type, struct hash_table *type_map, unsigned wave_size)
   {
      struct hash_entry *entry = _mesa_hash_table_search(type_map, orig_type);
   if (entry) {
         } else if (glsl_type_is_cmat(orig_type)) {
      struct glsl_cmat_description desc = *glsl_get_cmat_description(orig_type);
               } else if (glsl_type_is_array(orig_type)) {
      const struct glsl_type *elem_type = glsl_get_array_element(orig_type);
            if (elem_type == new_elem_type)
               } else if (glsl_type_is_struct(orig_type)) {
               bool change = false;
   for (unsigned i = 0; i < num_fields; ++i) {
                     if (field_type != new_field_type) {
      change = true;
                  if (!change)
                     for (unsigned i = 0; i < num_fields; ++i) {
                           const struct glsl_type *ret =
                  _mesa_hash_table_insert(type_map, orig_type, (void *)ret);
      } else
      }
      bool
   radv_nir_lower_cooperative_matrix(nir_shader *shader, unsigned wave_size)
   {
               if (!shader->info.cs.has_cooperative_matrix)
            struct nir_function *func = (struct nir_function *)exec_list_get_head_const(&shader->functions);
            nir_foreach_variable_with_modes (var, shader, nir_var_shader_temp) {
      const struct glsl_type *new_type = radv_nir_translate_matrix_type(var->type, type_map, wave_size);
   if (new_type != var->type) {
      var->type = new_type;
                  nir_foreach_function_temp_variable (var, func->impl) {
      const struct glsl_type *new_type = radv_nir_translate_matrix_type(var->type, type_map, wave_size);
   if (new_type != var->type) {
      var->type = new_type;
                           /* Iterate in reverse order so that lowering can still use the matrix types from the derefs before we change it. */
   nir_foreach_block_reverse (block, func->impl) {
      nir_foreach_instr_reverse_safe (instr, block) {
               switch (instr->type) {
   case nir_instr_type_intrinsic: {
      nir_intrinsic_instr *intr = nir_instr_as_intrinsic(instr);
   switch (intr->intrinsic) {
   case nir_intrinsic_cmat_length: {
      struct glsl_cmat_description desc = nir_intrinsic_cmat_desc(intr);
   unsigned len = radv_nir_cmat_length(desc, wave_size) / radv_nir_cmat_length_mul(desc);
   nir_def_rewrite_uses(&intr->def, nir_imm_int(&b, len));
   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_extract: {
                                                   nir_def_rewrite_uses(&intr->def, elem);
   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_insert: {
      nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[2].ssa);
   nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
                        nir_def *elem = intr->src[1].ssa;
   nir_def *r = nir_vector_insert(&b, src1, elem, index);
   nir_store_deref(&b, dst_deref, r, nir_component_mask(r->num_components));
   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_construct: {
                                    nir_store_deref(&b, dst_deref, r, nir_component_mask(r->num_components));
   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_load: {
                                                         /* A input is transposed */
                        unsigned length = radv_nir_cmat_length(desc, wave_size);
   unsigned mul = radv_nir_cmat_length_mul(desc);
   unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? wave_size : 16;
   nir_def *vars[16];
   if (mul > 1) {
      for (unsigned i = 0; i < length; ++i)
                                                                  if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
                                                      nir_deref_instr *iter_deref = nir_build_deref_ptr_as_array(&b, deref, col_offset);
   iter_deref =
                                    nir_def *mat = nir_vec(&b, vars, length);
   nir_store_deref(&b, dst_deref, mat, nir_component_mask(mat->num_components));
   nir_instr_remove(instr);
   progress = true;
      }
                                                                                                /* A input is transposed */
                        unsigned length = radv_nir_cmat_length(desc, wave_size);
   unsigned mul = radv_nir_cmat_length_mul(desc);
   unsigned lanes_per_iter = desc.use == GLSL_CMAT_USE_ACCUMULATOR ? wave_size : 16;
                                                                     if (layout == GLSL_MATRIX_LAYOUT_ROW_MAJOR) {
                                                      nir_deref_instr *iter_deref = nir_build_deref_ptr_as_array(&b, deref, col_offset);
   iter_deref =
                                                   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_muladd: {
      nir_def *A = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
                                       nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret,
         nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_unary_op: {
      nir_deref_instr *dst_deref = nir_instr_as_deref(intr->src[0].ssa->parent_instr);
   nir_deref_instr *src_deref = nir_instr_as_deref(intr->src[1].ssa->parent_instr);
   struct glsl_cmat_description desc = *glsl_get_cmat_description(dst_deref->type);
                        if (glsl_base_type_bit_size(src_desc.element_type) == 16 &&
      glsl_base_type_bit_size(desc.element_type) == 32 && desc.use == GLSL_CMAT_USE_ACCUMULATOR) {
   nir_def *components[NIR_MAX_VEC_COMPONENTS];
   for (unsigned i = 0; i * 2 < src->num_components; ++i) {
                                    if (glsl_base_type_bit_size(src_desc.element_type) == 32 &&
      glsl_base_type_bit_size(desc.element_type) == 16 && desc.use == GLSL_CMAT_USE_ACCUMULATOR) {
   nir_def *components[NIR_MAX_VEC_COMPONENTS];
   for (unsigned i = 0; i < ret->num_components; ++i) {
      components[i * 2] = nir_channel(&b, ret, i);
                        nir_store_deref(&b, dst_deref, ret, nir_component_mask(ret->num_components));
   nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_scalar_op: {
      nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
   nir_op op = nir_intrinsic_alu_op(intr);
   nir_def *ret = nir_build_alu2(&b, op, src1, intr->src[2].ssa);
   nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret,
         nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_binary_op: {
      nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
   nir_def *src2 = radv_nir_load_cmat(&b, wave_size, intr->src[2].ssa);
   nir_op op = nir_intrinsic_alu_op(intr);
   nir_def *ret = nir_build_alu2(&b, op, src1, src2);
   nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), ret,
         nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_bitcast: {
      nir_def *src1 = radv_nir_load_cmat(&b, wave_size, intr->src[1].ssa);
   nir_store_deref(&b, nir_instr_as_deref(intr->src[0].ssa->parent_instr), src1,
         nir_instr_remove(instr);
   progress = true;
      }
   case nir_intrinsic_cmat_copy: {
      nir_build_copy_deref(&b, intr->src[0].ssa, intr->src[1].ssa);
   nir_instr_remove(instr);
   progress = true;
      }
   default:
         }
      }
   case nir_instr_type_deref: {
      nir_deref_instr *deref = nir_instr_as_deref(instr);
   const struct glsl_type *new_type = radv_nir_translate_matrix_type(deref->type, type_map, wave_size);
   if (new_type != deref->type) {
      deref->type = new_type;
      }
      }
   default:
                                 if (progress) {
         } else {
                     }
