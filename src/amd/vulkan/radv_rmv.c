   /*
   * Copyright Â© 2022 Friedrich Vock
   *
   * Permission is hereby granted, free of charge, to any person obtaining a
   * copy of this software and associated documentation files (the "Software"),
   * to deal in the Software without restriction, including without limitation
   * the rights to use, copy, modify, merge, publish, distribute, sublicense,
   * and/or sell copies of the Software, and to permit persons to whom the
   * Software is furnished to do so, subject to the following conditions:
   *
   * The above copyright notice and this permission notice (including the next
   * paragraph) shall be included in all copies or substantial portions of the
   * Software.
   *
   * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
   * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
   * IN THE SOFTWARE.
   */
      #ifndef _WIN32
   #include <dirent.h>
   #include <unistd.h>
   #endif
   #include <errno.h>
   #include <fcntl.h>
   #include <stdio.h>
   #include <stdlib.h>
   #include "ac_gpu_info.h"
   #include "radv_private.h"
      #define RADV_FTRACE_INSTANCE_PATH "/sys/kernel/tracing/instances/amd_rmv"
      static FILE *
   open_event_file(const char *event_name, const char *event_filename, const char *mode)
   {
      char filename[2048];
   snprintf(filename, sizeof(filename), RADV_FTRACE_INSTANCE_PATH "/events/amdgpu/%s/%s", event_name, event_filename);
      }
      static bool
   set_event_tracing_enabled(const char *event_name, bool enabled)
   {
      FILE *file = open_event_file(event_name, "enable", "w");
   if (!file)
            size_t written_bytes = fwrite("1", 1, 1, file);
   fclose(file);
      }
      static uint16_t
   trace_event_id(const char *event_name)
   {
      /* id is 16-bit, so <= 65535 */
            FILE *file = open_event_file(event_name, "id", "r");
   if (!file)
            size_t read_bytes = fread(data, 1, 6, file);
            if (!read_bytes)
               }
      static void
   open_trace_pipe(uint32_t cpu_index, int *dst_fd)
   {
   #ifdef _WIN32
         #else
      char filename[2048];
   snprintf(filename, sizeof(filename), RADV_FTRACE_INSTANCE_PATH "/per_cpu/cpu%d/trace_pipe_raw", cpu_index);
   /* I/O to the pipe needs to be non-blocking, otherwise reading all available
   * data would block indefinitely by waiting for more data to be written to the pipe */
      #endif
   }
      /*
   * Kernel trace buffer parsing
   */
      struct trace_page_header {
      uint64_t timestamp;
      };
      enum trace_event_type { TRACE_EVENT_TYPE_PADDING = 29, TRACE_EVENT_TYPE_EXTENDED_DELTA, TRACE_EVENT_TYPE_TIMESTAMP };
      struct trace_event_header {
      uint32_t type_len : 5;
   uint32_t time_delta : 27;
   /* Only present if length is too big for type_len */
      };
      struct trace_event_common {
      unsigned short type;
   unsigned char flags;
   unsigned char preempt_count;
      };
      struct trace_event_amdgpu_vm_update_ptes {
      struct trace_event_common common;
   uint64_t start;
   uint64_t end;
   uint64_t flags;
   unsigned int num_ptes;
   uint64_t incr;
   int pid;
      };
      /* Represents a dynamic array of addresses in the ftrace buffer. */
   struct trace_event_address_array {
      uint16_t data_size;
   uint16_t reserved;
      };
      /* Possible flags for PTEs, taken from amdgpu_vm.h */
   #define AMDGPU_PTE_VALID  (1ULL << 0)
   #define AMDGPU_PTE_SYSTEM (1ULL << 1)
   #define AMDGPU_PTE_PRT    (1ULL << 51)
      /* The minimum size of a GPU page */
   #define MIN_GPU_PAGE_SIZE 4096
      static void
   emit_page_table_update_event(struct vk_memory_trace_data *data, bool is_apu, uint64_t timestamp,
         {
               uint64_t end_addr;
   /* There may be more updated PTEs than the ones reported in the ftrace buffer.
   * We choose the reported end virtual address here to report the correct total committed memory. */
   if (pte_index == event->num_ptes - 1)
         else
                  token.type = VK_RMV_TOKEN_TYPE_PAGE_TABLE_UPDATE;
   token.timestamp = timestamp;
   token.data.page_table_update.type = VK_RMV_PAGE_TABLE_UPDATE_TYPE_UPDATE;
   token.data.page_table_update.page_size = event->incr;
   token.data.page_table_update.page_count = (end_addr - start_addr) * MIN_GPU_PAGE_SIZE / event->incr;
   token.data.page_table_update.pid = event->common.pid;
   token.data.page_table_update.virtual_address = event->start * MIN_GPU_PAGE_SIZE + pte_index * event->incr;
   /* RMV expects mappings to system memory to have a physical address of 0.
   * Even with traces generated by AMDGPU-PRO, on APUs without dedicated VRAM everything seems to
   * be marked as "committed to system memory". */
            token.data.page_table_update.is_unmap = !(event->flags & (AMDGPU_PTE_VALID | AMDGPU_PTE_PRT));
      }
      static void
   evaluate_trace_event(struct radv_device *device, uint64_t timestamp, struct util_dynarray *tokens,
         {
      if (event->common.pid != getpid() && event->pid != getpid()) {
                           for (uint32_t i = 0; i < event->num_ptes; ++i)
      emit_page_table_update_event(&device->vk.memory_trace_data, !device->physical_device->rad_info.has_dedicated_vram,
   }
      static void
   append_trace_events(struct radv_device *device, int pipe_fd)
   {
      /* Assuming 4KB if os_get_page_size fails. */
   uint64_t page_size = 4096;
                     /*
   * Parse the trace ring buffer page by page.
   */
   char *page = (char *)malloc(page_size);
   if (!page) {
         }
   int64_t read_bytes;
   do {
      read_bytes = (int64_t)read(pipe_fd, page, page_size);
   if (read_bytes < (int64_t)sizeof(struct trace_page_header))
            struct trace_page_header *page_header = (struct trace_page_header *)page;
                     char *read_ptr = page + sizeof(struct trace_page_header);
   while (read_ptr - page < data_size) {
                     /* Handle special event type, see include/linux/ring_buffer.h in the
   * kernel source */
   switch (event_header->type_len) {
   case TRACE_EVENT_TYPE_PADDING:
      if (event_header->time_delta) {
      /* Specified size, skip past padding */
   read_ptr += event_header->excess_length;
   timestamp += event_header->time_delta;
      } else {
      /* Padding is until end of page, skip until next page */
   read_ptr = page + data_size;
         case TRACE_EVENT_TYPE_EXTENDED_DELTA:
      timestamp += event_header->time_delta;
   timestamp += (uint64_t)event_header->excess_length << 27ULL;
      case TRACE_EVENT_TYPE_TIMESTAMP:
      timestamp = event_header->time_delta;
   timestamp |= (uint64_t)event_header->excess_length << 27ULL;
      default:
                           /* If type_len is not one of the special types and not zero, it is
   * the data length / 4. */
   size_t length;
   struct trace_event_common *event;
   if (event_header->type_len) {
      length = event_header->type_len * 4 + 4;
   /* The length variable already contains event data in this case.
   */
      } else {
      length = event_header->excess_length + 4;
               if (event->type == device->memory_trace.ftrace_update_ptes_id)
                                    }
      static void
   close_pipe_fds(struct radv_device *device)
   {
      for (uint32_t i = 0; i < device->memory_trace.num_cpus; ++i) {
            }
      void
   radv_memory_trace_init(struct radv_device *device)
   {
   #ifndef _WIN32
      DIR *dir = opendir(RADV_FTRACE_INSTANCE_PATH);
   if (!dir) {
      fprintf(stderr,
         "radv: Couldn't initialize memory tracing: "
   "Can't access the tracing instance directory (%s)\n",
      }
                     char cpuinfo_line[1024];
   FILE *cpuinfo_file = fopen("/proc/cpuinfo", "r");
   uint32_t num_physical_cores;
   while (fgets(cpuinfo_line, sizeof(cpuinfo_line), cpuinfo_file)) {
      char *logical_core_string = strstr(cpuinfo_line, "siblings");
   if (logical_core_string)
         char *physical_core_string = strstr(cpuinfo_line, "cpu cores");
   if (physical_core_string)
      }
   if (!device->memory_trace.num_cpus)
                  FILE *clock_file = fopen(RADV_FTRACE_INSTANCE_PATH "/trace_clock", "w");
   if (!clock_file) {
      fprintf(stderr,
         "radv: Couldn't initialize memory tracing: "
   "Can't access the tracing control files (%s).\n",
               fprintf(clock_file, "mono");
                     if (!device->memory_trace.pipe_fds) {
         }
   for (uint32_t i = 0; i < device->memory_trace.num_cpus; ++i) {
               if (device->memory_trace.pipe_fds[i] == -1) {
      fprintf(stderr,
         "radv: Couldn't initialize memory tracing: "
   "Can't access the trace buffer pipes (%s).\n",
   for (i -= 1; i < device->memory_trace.num_cpus; --i) {
         }
                  device->memory_trace.ftrace_update_ptes_id = trace_event_id("amdgpu_vm_update_ptes");
   if (device->memory_trace.ftrace_update_ptes_id == (uint16_t)~0U) {
      fprintf(stderr,
         "radv: Couldn't initialize memory tracing: "
   "Can't access the trace event ID file (%s).\n",
               if (!set_event_tracing_enabled("amdgpu_vm_update_ptes", true)) {
      fprintf(stderr,
         "radv: Couldn't initialize memory tracing: "
   "Can't enable trace events (%s).\n",
               fprintf(stderr, "radv: Enabled Memory Trace.\n");
         error_pipes:
         error:
         #endif
   }
      static void
   fill_memory_info(const struct radeon_info *info, struct vk_rmv_memory_info *out_info, int32_t index)
   {
      switch (index) {
   case VK_RMV_MEMORY_LOCATION_DEVICE:
      out_info->physical_base_address = 0;
   out_info->size =
            case VK_RMV_MEMORY_LOCATION_DEVICE_INVISIBLE:
      out_info->physical_base_address = (uint64_t)info->vram_vis_size_kb * 1024ULL;
   out_info->size = info->all_vram_visible ? 0 : (uint64_t)info->vram_size_kb * 1024ULL;
      case VK_RMV_MEMORY_LOCATION_HOST: {
      uint64_t ram_size = -1U;
   os_get_total_physical_memory(&ram_size);
   out_info->physical_base_address = 0;
      } break;
   default:
            }
      static enum vk_rmv_memory_type
   memory_type_from_vram_type(uint32_t vram_type)
   {
      switch (vram_type) {
   case AMD_VRAM_TYPE_UNKNOWN:
         case AMD_VRAM_TYPE_DDR2:
         case AMD_VRAM_TYPE_DDR3:
         case AMD_VRAM_TYPE_DDR4:
         case AMD_VRAM_TYPE_GDDR5:
         case AMD_VRAM_TYPE_HBM:
         case AMD_VRAM_TYPE_GDDR6:
         case AMD_VRAM_TYPE_DDR5:
         case AMD_VRAM_TYPE_LPDDR4:
         case AMD_VRAM_TYPE_LPDDR5:
         default:
            }
      void
   radv_rmv_fill_device_info(const struct radv_physical_device *device, struct vk_rmv_device_info *info)
   {
               for (int32_t i = 0; i < VK_RMV_MEMORY_LOCATION_COUNT; ++i) {
                  if (rad_info->marketing_name)
         info->pcie_family_id = rad_info->family_id;
   info->pcie_revision_id = rad_info->pci_rev_id;
   info->pcie_device_id = rad_info->pci.dev;
   info->minimum_shader_clock = 0;
   info->maximum_shader_clock = rad_info->max_gpu_freq_mhz;
   info->vram_type = memory_type_from_vram_type(rad_info->vram_type);
   info->vram_bus_width = rad_info->memory_bus_width;
   info->vram_operations_per_clock = ac_memory_ops_per_clock(rad_info->vram_type);
   info->minimum_memory_clock = 0;
   info->maximum_memory_clock = rad_info->memory_freq_mhz;
      }
      void
   radv_rmv_collect_trace_events(struct radv_device *device)
   {
      for (uint32_t i = 0; i < device->memory_trace.num_cpus; ++i) {
            }
      void
   radv_memory_trace_finish(struct radv_device *device)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            set_event_tracing_enabled("amdgpu_vm_update_ptes", false);
      }
      /* The token lock must be held when entering _locked functions */
   static void
   log_resource_bind_locked(struct radv_device *device, uint64_t resource, struct radeon_winsys_bo *bo, uint64_t offset,
         {
      struct vk_rmv_resource_bind_token token = {0};
   token.address = bo->va + offset;
   token.is_system_memory = bo->initial_domain & RADEON_DOMAIN_GTT;
   token.size = size;
               }
      void
   radv_rmv_log_heap_create(struct radv_device *device, VkDeviceMemory heap, bool is_internal,
         {
      if (!device->vk.memory_trace_data.is_enabled)
                     /* Do not log zero-sized device memory objects. */
   if (!memory->alloc_size)
            radv_rmv_log_bo_allocate(device, memory->bo, memory->alloc_size, false);
            struct vk_rmv_resource_create_token token = {0};
   token.is_driver_internal = is_internal;
   token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)heap);
   token.type = VK_RMV_RESOURCE_TYPE_HEAP;
   token.heap.alignment = device->physical_device->rad_info.max_alignment;
   token.heap.size = memory->alloc_size;
   token.heap.heap_index = memory->heap_index;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &token);
   log_resource_bind_locked(device, (uint64_t)heap, memory->bo, 0, memory->alloc_size);
      }
      void
   radv_rmv_log_bo_allocate(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t size, bool is_internal)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            struct vk_rmv_virtual_allocate_token token = {0};
   token.address = bo->va;
   /* If all VRAM is visible, no bo will be in invisible memory. */
   token.is_in_invisible_vram = bo->vram_no_cpu_access && !device->physical_device->rad_info.all_vram_visible;
   token.preferred_domains = (enum vk_rmv_kernel_memory_domain)bo->initial_domain;
   token.is_driver_internal = is_internal;
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_VIRTUAL_ALLOCATE, &token);
   radv_rmv_collect_trace_events(device);
      }
      void
   radv_rmv_log_bo_destroy(struct radv_device *device, struct radeon_winsys_bo *bo)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            struct vk_rmv_virtual_free_token token = {0};
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_VIRTUAL_FREE, &token);
   radv_rmv_collect_trace_events(device);
      }
      void
   radv_rmv_log_buffer_bind(struct radv_device *device, VkBuffer _buffer)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            RADV_FROM_HANDLE(radv_buffer, buffer, _buffer);
   simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   log_resource_bind_locked(device, (uint64_t)_buffer, buffer->bo, buffer->offset, buffer->vk.size);
      }
      void
   radv_rmv_log_image_create(struct radv_device *device, const VkImageCreateInfo *create_info, bool is_internal,
         {
      if (!device->vk.memory_trace_data.is_enabled)
                     simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token token = {0};
   token.is_driver_internal = is_internal;
   token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_image);
   token.type = VK_RMV_RESOURCE_TYPE_IMAGE;
   token.image.create_flags = create_info->flags;
   token.image.usage_flags = create_info->usage;
   token.image.type = create_info->imageType;
   token.image.extent = create_info->extent;
   token.image.format = create_info->format;
   token.image.num_mips = create_info->mipLevels;
   token.image.num_slices = create_info->arrayLayers;
   token.image.tiling = create_info->tiling;
   token.image.alignment_log2 = util_logbase2(image->alignment);
   token.image.log2_samples = util_logbase2(image->vk.samples);
   token.image.log2_storage_samples = util_logbase2(image->vk.samples);
   token.image.metadata_alignment_log2 = image->planes[0].surface.meta_alignment_log2;
   token.image.image_alignment_log2 = image->planes[0].surface.alignment_log2;
   token.image.size = image->size;
   token.image.metadata_size = image->planes[0].surface.meta_size;
   token.image.metadata_header_size = 0;
   token.image.metadata_offset = image->planes[0].surface.meta_offset;
   token.image.metadata_header_offset = image->planes[0].surface.meta_offset;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &token);
      }
      void
   radv_rmv_log_image_bind(struct radv_device *device, VkImage _image)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            RADV_FROM_HANDLE(radv_image, image, _image);
   simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   log_resource_bind_locked(device, (uint64_t)_image, image->bindings[0].bo, image->bindings[0].offset, image->size);
      }
      void
   radv_rmv_log_query_pool_create(struct radv_device *device, VkQueryPool _pool, bool is_internal)
   {
      if (!device->vk.memory_trace_data.is_enabled)
                     if (pool->vk.query_type != VK_QUERY_TYPE_OCCLUSION && pool->vk.query_type != VK_QUERY_TYPE_PIPELINE_STATISTICS &&
      pool->vk.query_type != VK_QUERY_TYPE_TRANSFORM_FEEDBACK_STREAM_EXT)
                  simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = is_internal;
   create_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_pool);
   create_token.type = VK_RMV_RESOURCE_TYPE_QUERY_HEAP;
   create_token.query_pool.type = pool->vk.query_type;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   log_resource_bind_locked(device, (uint64_t)_pool, pool->bo, 0, pool->size);
      }
      void
   radv_rmv_log_command_buffer_bo_create(struct radv_device *device, struct radeon_winsys_bo *bo, uint32_t executable_size,
         {
      if (!device->vk.memory_trace_data.is_enabled)
            /* Only one of executable_size, data_size and scratch_size should be > 0 */
   /* TODO: Trace CS BOs for executable data */
                              simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = true;
   create_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, upload_resource_identifier);
   create_token.type = VK_RMV_RESOURCE_TYPE_COMMAND_ALLOCATOR;
   create_token.command_buffer.preferred_domain = (enum vk_rmv_kernel_memory_domain)device->ws->cs_domain(device->ws);
   create_token.command_buffer.executable_size = executable_size;
   create_token.command_buffer.app_available_executable_size = executable_size;
   create_token.command_buffer.embedded_data_size = data_size;
   create_token.command_buffer.app_available_embedded_data_size = data_size;
   create_token.command_buffer.scratch_size = scratch_size;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   log_resource_bind_locked(device, upload_resource_identifier, bo, 0, size);
   simple_mtx_unlock(&device->vk.memory_trace_data.token_mtx);
      }
      void
   radv_rmv_log_command_buffer_bo_destroy(struct radv_device *device, struct radeon_winsys_bo *bo)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_destroy_token destroy_token = {0};
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_DESTROY, &destroy_token);
   vk_rmv_destroy_resource_id_locked(&device->vk, (uint64_t)(uintptr_t)bo);
   simple_mtx_unlock(&device->vk.memory_trace_data.token_mtx);
   radv_rmv_log_bo_destroy(device, bo);
      }
      void
   radv_rmv_log_border_color_palette_create(struct radv_device *device, struct radeon_winsys_bo *bo)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            radv_rmv_log_bo_allocate(device, bo, RADV_BORDER_COLOR_BUFFER_SIZE, true);
   simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
            struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = true;
   create_token.resource_id = resource_id;
   create_token.type = VK_RMV_RESOURCE_TYPE_BORDER_COLOR_PALETTE;
   /*
   * We have 4096 entries, but the corresponding RMV token only has 8 bits.
   */
            struct vk_rmv_resource_bind_token bind_token;
   bind_token.address = bo->va;
   bind_token.is_system_memory = false;
   bind_token.resource_id = resource_id;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_BIND, &bind_token);
   simple_mtx_unlock(&device->vk.memory_trace_data.token_mtx);
      }
      void
   radv_rmv_log_border_color_palette_destroy(struct radv_device *device, struct radeon_winsys_bo *bo)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_destroy_token token = {0};
   /* same resource id as the create token */
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_DESTROY, &token);
   simple_mtx_unlock(&device->vk.memory_trace_data.token_mtx);
      }
      void
   radv_rmv_log_sparse_add_residency(struct radv_device *device, struct radeon_winsys_bo *src_bo, uint64_t offset)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            struct vk_rmv_resource_reference_token token = {0};
   token.virtual_address = src_bo->va + offset;
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_REFERENCE, &token);
   radv_rmv_collect_trace_events(device);
      }
      void
   radv_rmv_log_sparse_remove_residency(struct radv_device *device, struct radeon_winsys_bo *src_bo, uint64_t offset)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            struct vk_rmv_resource_reference_token token = {0};
   token.virtual_address = src_bo->va + offset;
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_REFERENCE, &token);
   radv_rmv_collect_trace_events(device);
      }
      void
   radv_rmv_log_descriptor_pool_create(struct radv_device *device, const VkDescriptorPoolCreateInfo *create_info,
         {
      if (!device->vk.memory_trace_data.is_enabled)
                     if (pool->bo) {
      radv_rmv_log_bo_allocate(device, pool->bo, pool->size, is_internal);
               simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = false;
   create_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_pool);
   create_token.type = VK_RMV_RESOURCE_TYPE_DESCRIPTOR_POOL;
   create_token.descriptor_pool.max_sets = create_info->maxSets;
   create_token.descriptor_pool.pool_size_count = create_info->poolSizeCount;
   /* Using vk_rmv_token_pool_alloc frees the allocation automatically when the trace is done. */
   create_token.descriptor_pool.pool_sizes = malloc(create_info->poolSizeCount * sizeof(VkDescriptorPoolSize));
   if (!create_token.descriptor_pool.pool_sizes)
            memcpy(create_token.descriptor_pool.pool_sizes, create_info->pPoolSizes,
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
            if (pool->bo) {
      simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_bind_token bind_token;
   bind_token.address = pool->bo->va;
   bind_token.is_system_memory = false;
   bind_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_pool);
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_BIND, &bind_token);
         }
      void
   radv_rmv_log_graphics_pipeline_create(struct radv_device *device, struct radv_pipeline *pipeline, bool is_internal)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            VkPipeline _pipeline = radv_pipeline_to_handle(pipeline);
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = is_internal;
   create_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_pipeline);
   create_token.type = VK_RMV_RESOURCE_TYPE_PIPELINE;
   create_token.pipeline.is_internal = is_internal;
   create_token.pipeline.hash_lo = pipeline->pipeline_hash;
   create_token.pipeline.is_ngg = graphics_pipeline->is_ngg;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   for (unsigned s = 0; s < MESA_VULKAN_SHADER_STAGES; s++) {
               if (!shader)
               }
      }
      void
   radv_rmv_log_compute_pipeline_create(struct radv_device *device, struct radv_pipeline *pipeline, bool is_internal)
   {
      if (!device->vk.memory_trace_data.is_enabled)
                     VkShaderStageFlagBits active_stages =
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = is_internal;
   create_token.resource_id = vk_rmv_get_resource_id_locked(&device->vk, (uint64_t)_pipeline);
   create_token.type = VK_RMV_RESOURCE_TYPE_PIPELINE;
   create_token.pipeline.is_internal = is_internal;
   create_token.pipeline.hash_lo = pipeline->pipeline_hash;
   create_token.pipeline.is_ngg = false;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   struct radv_shader *shader = pipeline->shaders[vk_to_mesa_shader_stage(active_stages)];
   log_resource_bind_locked(device, (uint64_t)_pipeline, shader->bo, shader->alloc->offset, shader->alloc->size);
      }
      void
   radv_rmv_log_event_create(struct radv_device *device, VkEvent _event, VkEventCreateFlags flags, bool is_internal)
   {
      if (!device->vk.memory_trace_data.is_enabled)
                     radv_rmv_log_bo_allocate(device, event->bo, 8, is_internal);
   simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_create_token create_token = {0};
   create_token.is_driver_internal = is_internal;
   create_token.type = VK_RMV_RESOURCE_TYPE_GPU_EVENT;
   create_token.event.flags = flags;
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_CREATE, &create_token);
   log_resource_bind_locked(device, (uint64_t)_event, event->bo, 0, 8);
            if (event->map)
      }
      void
   radv_rmv_log_submit(struct radv_device *device, enum amd_ip_type type)
   {
      if (!device->vk.memory_trace_data.is_enabled)
            switch (type) {
   case AMD_IP_GFX:
      vk_rmv_log_misc_token(&device->vk, VK_RMV_MISC_EVENT_TYPE_SUBMIT_GRAPHICS);
      case AMD_IP_COMPUTE:
      vk_rmv_log_misc_token(&device->vk, VK_RMV_MISC_EVENT_TYPE_SUBMIT_COMPUTE);
      case AMD_IP_SDMA:
      vk_rmv_log_misc_token(&device->vk, VK_RMV_MISC_EVENT_TYPE_SUBMIT_COPY);
      default:
            }
      void
   radv_rmv_log_resource_destroy(struct radv_device *device, uint64_t handle)
   {
      if (!device->vk.memory_trace_data.is_enabled || handle == 0)
            simple_mtx_lock(&device->vk.memory_trace_data.token_mtx);
   struct vk_rmv_resource_destroy_token token = {0};
            vk_rmv_emit_token(&device->vk.memory_trace_data, VK_RMV_TOKEN_TYPE_RESOURCE_DESTROY, &token);
   vk_rmv_destroy_resource_id_locked(&device->vk, handle);
      }
