   /*
   * Copyright Â© 2015 Broadcom
   *
   * Permission is hereby granted, free of charge, to any person obtaining a
   * copy of this software and associated documentation files (the "Software"),
   * to deal in the Software without restriction, including without limitation
   * the rights to use, copy, modify, merge, publish, distribute, sublicense,
   * and/or sell copies of the Software, and to permit persons to whom the
   * Software is furnished to do so, subject to the following conditions:
   *
   * The above copyright notice and this permission notice (including the next
   * paragraph) shall be included in all copies or substantial portions of the
   * Software.
   *
   * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
   * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
   * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
   * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
   * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
   * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
   * IN THE SOFTWARE.
   */
      #include "vc4_qir.h"
   #include "compiler/nir/nir_builder.h"
   #include "util/format/u_format.h"
   #include "util/u_helpers.h"
      /**
   * Walks the NIR generated by TGSI-to-NIR or GLSL-to-NIR to lower its io
   * intrinsics into something amenable to the VC4 architecture.
   *
   * Currently, it splits VS inputs and uniforms into scalars, drops any
   * non-position outputs in coordinate shaders, and fixes up the addressing on
   * indirect uniform loads.  FS input and VS output scalarization is handled by
   * nir_lower_io_to_scalar().
   */
      static void
   replace_intrinsic_with_vec(nir_builder *b, nir_intrinsic_instr *intr,
         {
            /* Batch things back together into a vector.  This will get split by
      * the later ALU scalarization pass.
               /* Replace the old intrinsic with a reference to our reconstructed
      * vector.
      nir_def_rewrite_uses(&intr->def, vec);
   }
      static nir_def *
   vc4_nir_unpack_8i(nir_builder *b, nir_def *src, unsigned chan)
   {
         return nir_ubitfield_extract(b,
               }
      /** Returns the 16 bit field as a sign-extended 32-bit value. */
   static nir_def *
   vc4_nir_unpack_16i(nir_builder *b, nir_def *src, unsigned chan)
   {
         return nir_ibitfield_extract(b,
               }
      /** Returns the 16 bit field as an unsigned 32 bit value. */
   static nir_def *
   vc4_nir_unpack_16u(nir_builder *b, nir_def *src, unsigned chan)
   {
         if (chan == 0) {
         } else {
         }
      static nir_def *
   vc4_nir_unpack_8f(nir_builder *b, nir_def *src, unsigned chan)
   {
         }
      static nir_def *
   vc4_nir_get_vattr_channel_vpm(struct vc4_compile *c,
                           {
         const struct util_format_channel_description *chan =
                  if (swiz > PIPE_SWIZZLE_W) {
         } else if (chan->size == 32 && chan->type == UTIL_FORMAT_TYPE_FLOAT) {
         } else if (chan->size == 32 && chan->type == UTIL_FORMAT_TYPE_SIGNED) {
            if (chan->normalized) {
            return nir_fmul_imm(b,
      } else {
      } else if (chan->size == 8 &&
               (chan->type == UTIL_FORMAT_TYPE_UNSIGNED ||
      nir_def *vpm = vpm_reads[0];
   if (chan->type == UTIL_FORMAT_TYPE_SIGNED) {
            temp = nir_ixor(b, vpm, nir_imm_int(b, 0x80808080));
   if (chan->normalized) {
         return nir_fadd_imm(b, nir_fmul_imm(b,
               } else {
         return nir_fadd_imm(b,
                  } else {
            if (chan->normalized) {
         } else {
   } else if (chan->size == 16 &&
                                 /* Note that UNPACK_16F eats a half float, not ints, so we use
   * UNPACK_16_I for all of these.
   */
   if (chan->type == UTIL_FORMAT_TYPE_SIGNED) {
            temp = nir_i2f32(b, vc4_nir_unpack_16i(b, vpm, swiz & 1));
   if (chan->normalized) {
         } else {
      } else {
            temp = nir_i2f32(b, vc4_nir_unpack_16u(b, vpm, swiz & 1));
   if (chan->normalized) {
         } else {
   } else {
         }
      static void
   vc4_nir_lower_vertex_attr(struct vc4_compile *c, nir_builder *b,
         {
                  int attr = nir_intrinsic_base(intr);
   enum pipe_format format = c->vs_key->attr_formats[attr];
            /* We only accept direct outputs and TGSI only ever gives them to us
      * with an offset value of 0.
               /* Generate dword loads for the VPM values (Since these intrinsics may
      * be reordered, the actual reads will be generated at the top of the
   * shader by ntq_setup_inputs().
      nir_def *vpm_reads[4];
   for (int i = 0; i < align(attr_size, 4) / 4; i++)
                        bool format_warned = false;
   const struct util_format_description *desc =
            nir_def *dests[4];
   for (int i = 0; i < intr->num_components; i++) {
                                 if (!dests[i]) {
            if (!format_warned) {
         fprintf(stderr,
                        }
      static void
   vc4_nir_lower_fs_input(struct vc4_compile *c, nir_builder *b,
         {
                  if (nir_intrinsic_base(intr) >= VC4_NIR_TLB_COLOR_READ_INPUT &&
         nir_intrinsic_base(intr) < (VC4_NIR_TLB_COLOR_READ_INPUT +
                        nir_variable *input_var =
                                 /* Lower away point coordinates, and fix up PNTC. */
   if (util_varying_is_point_coord(input_var->data.location,
                                    switch (comp) {
   case 0:
   case 1:
            /* If we're not rendering points, we need to set a
   * defined value for the input that would come from
   * PNTC.
   */
   if (!c->fs_key->is_points)
      case 2:
               case 3:
                                       if (result != &intr->def) {
            nir_def_rewrite_uses_after(&intr->def,
   }
      static void
   vc4_nir_lower_output(struct vc4_compile *c, nir_builder *b,
         {
         nir_variable *output_var =
                        if (c->stage == QSTAGE_COORD &&
         output_var->data.location != VARYING_SLOT_POS &&
   output_var->data.location != VARYING_SLOT_PSIZ) {
         }
      static void
   vc4_nir_lower_uniform(struct vc4_compile *c, nir_builder *b,
         {
                  /* Generate scalar loads equivalent to the original vector. */
   nir_def *dests[4];
   for (unsigned i = 0; i < intr->num_components; i++) {
            nir_intrinsic_instr *intr_comp =
                              /* Convert the uniform offset to bytes.  If it happens
   * to be a constant, constant-folding will clean up
   * the shift for us.
   */
   nir_intrinsic_set_base(intr_comp,
                                                            }
      static void
   vc4_nir_lower_io_instr(struct vc4_compile *c, nir_builder *b,
         {
         if (instr->type != nir_instr_type_intrinsic)
                  switch (intr->intrinsic) {
   case nir_intrinsic_load_input:
            if (c->stage == QSTAGE_FRAG)
                     case nir_intrinsic_store_output:
                  case nir_intrinsic_load_uniform:
                  case nir_intrinsic_load_user_clip_plane:
   default:
         }
      static bool
   vc4_nir_lower_io_impl(struct vc4_compile *c, nir_function_impl *impl)
   {
                  nir_foreach_block(block, impl) {
                        nir_metadata_preserve(impl, nir_metadata_block_index |
            }
      void
   vc4_nir_lower_io(nir_shader *s, struct vc4_compile *c)
   {
         nir_foreach_function_impl(impl, s) {
         }
